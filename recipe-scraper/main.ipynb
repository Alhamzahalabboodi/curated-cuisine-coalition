{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e615f39e-f30e-45e1-a79c-be250c9772f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b23301f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Scraping Data\n",
    "def scrape_recipe(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Ingredients\n",
    "    ingredients = soup.find(\"div\", class_=\"loc section-content section__content\")\n",
    "    if ingredients:\n",
    "        ingredients = ingredients.text.strip().replace('\\n', '<br>')\n",
    "    else:\n",
    "        ingredients = \"Ingredients not found\"\n",
    "    \n",
    "    # Description\n",
    "    description = soup.find(\"h2\", class_=\"heading__subtitle\")\n",
    "    if description:\n",
    "        description = description.text.strip().replace('\\n', '<br>')\n",
    "    else:\n",
    "        description = \"Description not found\"\n",
    "\n",
    "    # Recipe Facts\n",
    "    recipe_facts = soup.find(\"div\", id=\"project-meta_1-0\", class_=\"comp project-meta\")\n",
    "    if recipe_facts:\n",
    "        recipe_facts = recipe_facts.text.strip().replace('\\n', '<br>')\n",
    "    else:\n",
    "        recipe_facts = \"Recipe facts not found\"\n",
    "\n",
    "    # Directions\n",
    "    directions = soup.find(\"div\", id=\"structured-project__steps_1-0\", class_=\"comp text-passage structured-content structured-project__steps mntl-sc-page mntl-block\")\n",
    "    if directions:\n",
    "        directions = directions.text.strip().replace('\\n', '<br>')\n",
    "    else:\n",
    "        directions = \"Directions not found\"\n",
    "\n",
    "    # Nutrition Facts\n",
    "    nutrition_facts = soup.find(\"tbody\", class_=\"nutrition-info__table--body\")\n",
    "    if nutrition_facts:\n",
    "        nutrition_facts = nutrition_facts.text.strip().replace('\\n', '<br>')\n",
    "    else:\n",
    "        nutrition_facts = \"Nutrition facts not found\"\n",
    "    \n",
    "    # Rating\n",
    "    ratings = soup.find(\"p\", id=\"recipe-rating_1-0\", class_=\"comp recipe-rating text-block\")\n",
    "    if ratings:\n",
    "        ratings = ratings.text.strip()\n",
    "    else:\n",
    "        ratings = \"Ratings not found\"\n",
    "\n",
    "    # Tags\n",
    "    tags = soup.find(\"ul\", id=\"link-list_1-0\", class_=\"comp tag-nav__list link-list\")\n",
    "    if tags:\n",
    "        tags = [tag.text.strip() for tag in tags.find_all(\"a\")]\n",
    "        tags = ', '.join(tags)\n",
    "    else:\n",
    "        tags = \"Tags not found\"\n",
    "\n",
    "    # Title\n",
    "    title = soup.find(\"h1\", class_=\"heading__title\")\n",
    "    if title:\n",
    "        title = title.text.strip().replace('\\n', '<br>')\n",
    "    else:\n",
    "        title = \"Title not found\"\n",
    "\n",
    "    # Number of servings\n",
    "    nutrition_label = soup.find(\"div\", class_=\"nutrition-label\")\n",
    "    if nutrition_label:\n",
    "        number_of_servings = nutrition_label.find('table').find('thead').find('tr').find_next_sibling().text[10:]\n",
    "    else:\n",
    "        number_of_servings = \"Number of servings not found\"\n",
    "    \n",
    "    # Calories\n",
    "    if nutrition_label:\n",
    "        table_body = nutrition_label.find('table').find('tbody').find('tr')\n",
    "        calories = table_body.find_next_sibling().find('td').find_next_sibling().text\n",
    "    else:\n",
    "        calories = \"Calories not found\"\n",
    "\n",
    "    # Total Fats\n",
    "    if nutrition_label:\n",
    "        tot_fat = table_body.find_next_sibling().find_next_sibling().find_next_sibling().find('td').text[10:]\n",
    "    else:\n",
    "        tot_fat = \"Total Fats not found\"\n",
    "\n",
    "    # Saturated Fats\n",
    "    if nutrition_label:\n",
    "        sat_fat = table_body.find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find('td').text[14:]\n",
    "    else:\n",
    "        sat_fat = \"Saturated Fats not found\"\n",
    "\n",
    "    # Cholesterol\n",
    "    if nutrition_label:\n",
    "        cholesterol = table_body.find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find('td').text[12:]\n",
    "    else:\n",
    "        cholesterol = \"Cholesterol not found\"\n",
    "\n",
    "    # Sodium\n",
    "    if nutrition_label:\n",
    "        sodium = table_body.find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find('td').text[7:]\n",
    "    else:\n",
    "        sodium = \"Sodium not found\"\n",
    "\n",
    "    # Total Carbohydrates\n",
    "    if nutrition_label:\n",
    "        tot_carb = table_body.find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find('td').text[19:]\n",
    "    else:\n",
    "        tot_carb = \"Total Carbohydrates not found\"\n",
    "\n",
    "    # Dietary Fibers\n",
    "    if nutrition_label:\n",
    "        diet_fib = table_body.find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find('td').text[14:]\n",
    "    else:\n",
    "        diet_fib = \"Dietary Fibers not found\"\n",
    "\n",
    "    # Total Sugars\n",
    "    if nutrition_label:\n",
    "        tot_sugar = table_body.find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find('td').text[13:]\n",
    "    else:\n",
    "        tot_sugar = \"Total Sugars not found\"\n",
    "\n",
    "    # Protein\n",
    "    if nutrition_label:\n",
    "        protein = table_body.find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find_next_sibling().find('td').text[7:]\n",
    "    else:\n",
    "        protein = \"Protein not found\"\n",
    "\n",
    "    # Vitamin C\n",
    "    if nutrition_label:\n",
    "        table_foot = nutrition_label.find('table').find('tfoot').find('tr')\n",
    "    if nutrition_label:\n",
    "        vit_c = table_foot.find('td').text[10:]\n",
    "    else:\n",
    "        vit_c = \"Vitamin C not found\"\n",
    "\n",
    "    # Calcium\n",
    "    if nutrition_label:\n",
    "        calcium = table_foot.find_next_sibling().find('td').text[8:]\n",
    "    else:\n",
    "        calcium = \"Calcium not found\"\n",
    "\n",
    "    # Iron\n",
    "    if nutrition_label:\n",
    "        iron = table_foot.find_next_sibling().find_next_sibling().find('td').text[5:]\n",
    "    else:\n",
    "        iron = \"Iron not found\"\n",
    "\n",
    "    # potassium\n",
    "    if nutrition_label:\n",
    "        potassium = table_foot.find_next_sibling().find_next_sibling().find_next_sibling().find('td').text[10:]\n",
    "    else:\n",
    "        potassium = \"Potassium not found\"\n",
    "\n",
    "    # Image Link\n",
    "    image_data = soup.find('div', class_= 'img-placeholder')\n",
    "    if image_data:\n",
    "        try:\n",
    "            image_link = image_data.find('img')['src']\n",
    "        except:\n",
    "            image_link = image_data.find('img')['data-src']\n",
    "    else:\n",
    "        image_link = \"Image not found\"\n",
    "    \n",
    "    \n",
    "    return title, ratings, description, ingredients, recipe_facts, directions, nutrition_facts, tags, number_of_servings, calories, tot_fat, sat_fat, cholesterol, sodium, tot_carb, diet_fib, tot_sugar, protein, vit_c, calcium, iron, potassium, image_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "70d47f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV of urls to scrape\n",
    "url_df = pd.read_csv('SeriousEatsCSV.csv')\n",
    "urls = []\n",
    "for url in url_df['loc']:\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d01e7404",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ander\\databootcamp\\recipe-scraper\\main.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/databootcamp/recipe-scraper/main.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/databootcamp/recipe-scraper/main.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m urls:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ander/databootcamp/recipe-scraper/main.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     title, ratings, description, ingredients, recipe_facts, directions, nutrition_facts, tags, number_of_servings, calories, tot_fat, sat_fat, cholesterol, sodium, tot_carb, diet_fib, tot_sugar, protein, vit_c, calcium, iron, potassium, image_link \u001b[39m=\u001b[39m scrape_recipe(url)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/databootcamp/recipe-scraper/main.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     results\u001b[39m.\u001b[39mappend([title, ratings, description, tags, ingredients, recipe_facts, directions, nutrition_facts, number_of_servings, calories, tot_fat, sat_fat, cholesterol, sodium, tot_carb, diet_fib, tot_sugar, protein, vit_c, calcium, iron, potassium, image_link])\n",
      "\u001b[1;32mc:\\Users\\ander\\databootcamp\\recipe-scraper\\main.ipynb Cell 4\u001b[0m in \u001b[0;36mscrape_recipe\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/databootcamp/recipe-scraper/main.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscrape_recipe\u001b[39m(url):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/databootcamp/recipe-scraper/main.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ander/databootcamp/recipe-scraper/main.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39;49mcontent, \u001b[39m\"\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/databootcamp/recipe-scraper/main.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Ingredients\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ander/databootcamp/recipe-scraper/main.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     ingredients \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m\"\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloc section-content section__content\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ander\\anaconda3\\envs\\PythonData38\\lib\\site-packages\\bs4\\__init__.py:333\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[0;32m    332\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed()\n\u001b[0;32m    334\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ander\\anaconda3\\envs\\PythonData38\\lib\\site-packages\\bs4\\__init__.py:451\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mfeed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[0;32m    452\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[1;32mc:\\Users\\ander\\anaconda3\\envs\\PythonData38\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:399\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    397\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[0;32m    398\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     parser\u001b[39m.\u001b[39;49mfeed(markup)\n\u001b[0;32m    400\u001b[0m     parser\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    401\u001b[0m \u001b[39mexcept\u001b[39;00m HTMLParseError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ander\\anaconda3\\envs\\PythonData38\\lib\\html\\parser.py:111\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[1;32m--> 111\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoahead(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ander\\anaconda3\\envs\\PythonData38\\lib\\html\\parser.py:171\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m startswith(\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m, i):\n\u001b[0;32m    170\u001b[0m     \u001b[39mif\u001b[39;00m starttagopen\u001b[39m.\u001b[39mmatch(rawdata, i): \u001b[39m# < + letter\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_starttag(i)\n\u001b[0;32m    172\u001b[0m     \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[0;32m    173\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[1;32mc:\\Users\\ander\\anaconda3\\envs\\PythonData38\\lib\\html\\parser.py:345\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[0;32m    344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_starttag(tag, attrs)\n\u001b[0;32m    346\u001b[0m     \u001b[39mif\u001b[39;00m tag \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[0;32m    347\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_cdata_mode(tag)\n",
      "File \u001b[1;32mc:\\Users\\ander\\anaconda3\\envs\\PythonData38\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:154\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[1;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39m#print(\"START\", name)\u001b[39;00m\n\u001b[0;32m    153\u001b[0m sourceline, sourcepos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetpos()\n\u001b[1;32m--> 154\u001b[0m tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoup\u001b[39m.\u001b[39;49mhandle_starttag(\n\u001b[0;32m    155\u001b[0m     name, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, attr_dict, sourceline\u001b[39m=\u001b[39;49msourceline,\n\u001b[0;32m    156\u001b[0m     sourcepos\u001b[39m=\u001b[39;49msourcepos\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m tag \u001b[39mand\u001b[39;00m tag\u001b[39m.\u001b[39mis_empty_element \u001b[39mand\u001b[39;00m handle_empty_element:\n\u001b[0;32m    159\u001b[0m     \u001b[39m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[39m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# events for tags of this name.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ander\\anaconda3\\envs\\PythonData38\\lib\\site-packages\\bs4\\__init__.py:721\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[1;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagStack) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    717\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only\u001b[39m.\u001b[39mtext\n\u001b[0;32m    718\u001b[0m          \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only\u001b[39m.\u001b[39msearch_tag(name, attrs))):\n\u001b[0;32m    719\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49melement_classes\u001b[39m.\u001b[39;49mget(Tag, Tag)(\n\u001b[0;32m    722\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder, name, namespace, nsprefix, attrs,\n\u001b[0;32m    723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrentTag, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_most_recent_element,\n\u001b[0;32m    724\u001b[0m     sourceline\u001b[39m=\u001b[39;49msourceline, sourcepos\u001b[39m=\u001b[39;49msourcepos,\n\u001b[0;32m    725\u001b[0m     namespaces\u001b[39m=\u001b[39;49mnamespaces\n\u001b[0;32m    726\u001b[0m )\n\u001b[0;32m    727\u001b[0m \u001b[39mif\u001b[39;00m tag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     \u001b[39mreturn\u001b[39;00m tag\n",
      "File \u001b[1;32mc:\\Users\\ander\\anaconda3\\envs\\PythonData38\\lib\\site-packages\\bs4\\element.py:1228\u001b[0m, in \u001b[0;36mTag.__init__\u001b[1;34m(self, parser, builder, name, namespace, prefix, attrs, parent, previous, is_xml, sourceline, sourcepos, can_be_empty_element, cdata_list_attributes, preserve_whitespace_tags, interesting_string_types, namespaces)\u001b[0m\n\u001b[0;32m   1224\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparser_class \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1226\u001b[0m     \u001b[39m# We don't actually store the parser object: that lets extracted\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m     \u001b[39m# chunks be garbage-collected.\u001b[39;00m\n\u001b[1;32m-> 1228\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparser_class \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\n\u001b[0;32m   1229\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo value provided for new tag\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms name.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Scrape URL for information\n",
    "results = []\n",
    "for url in urls:\n",
    "    title, ratings, description, ingredients, recipe_facts, directions, nutrition_facts, tags, number_of_servings, calories, tot_fat, sat_fat, cholesterol, sodium, tot_carb, diet_fib, tot_sugar, protein, vit_c, calcium, iron, potassium, image_link = scrape_recipe(url)\n",
    "    results.append([title, ratings, description, tags, ingredients, recipe_facts, directions, nutrition_facts, number_of_servings, calories, tot_fat, sat_fat, cholesterol, sodium, tot_carb, diet_fib, tot_sugar, protein, vit_c, calcium, iron, potassium, image_link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b66981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with Information we scraped\n",
    "df = pd.DataFrame(results, columns=[\"Title\", \"Rating\", \"Description\", \"Tags\", \"Ingredients\", \"Recipe Facts\",\n",
    "                                    \"Directions\", \"Nutrition Facts\", 'Number of Servings',\n",
    "                                    'Calories', 'Total Fat', 'Saturated Fat', 'Cholesterol', 'Sodium',\n",
    "                                    'Total Carbohydrate', 'Dietary Fiber', 'Total Sugars', 'Protein',\n",
    "                                    'Vitamin C', 'Calcium', 'Iron', 'Potassium', 'Image Link'])\n",
    "df = df.replace({'<br>': '\\n'}, regex=True)\n",
    "df = df.replace({'\\n+': '\\n'}, regex=True)\n",
    "df = df.replace({'\\n+': ' '}, regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Tags for keywords\n",
    "course_keywords = ['breakfast', 'brunch', 'main', 'snack', 'appetizer', 'salad', 'side', 'dessert', 'condiment', 'sauce']\n",
    "cuisine_keywords = ['african', 'asian', 'caribbean', 'central american', 'europe', 'middle eastern', 'north american', 'oceanic', 'south american', 'world']\n",
    "diet_keywords = ['dairy-free', 'gluten-free', 'vegan', 'vegetarian']\n",
    "\n",
    "course_list = []\n",
    "cuisine_list = []\n",
    "diet_list = []\n",
    "\n",
    "for tag in df['Tags']:\n",
    "    tag = tag.lower()\n",
    "    temp = []\n",
    "    for keyword in course_keywords:\n",
    "        if keyword in tag:\n",
    "            if keyword not in temp:\n",
    "                temp.append(keyword)\n",
    "    course_list.append(temp)\n",
    "\n",
    "    temp = []\n",
    "    for keyword in cuisine_keywords:\n",
    "        if keyword in tag:\n",
    "            if keyword not in temp:\n",
    "                temp.append(keyword)\n",
    "    cuisine_list.append(temp)\n",
    "\n",
    "    temp = []\n",
    "    for keyword in diet_keywords:\n",
    "        if keyword in tag:\n",
    "            if keyword not in temp:\n",
    "                temp.append(keyword)\n",
    "    diet_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e62680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Series to DataFrame\n",
    "df['Course Keywords'] = course_list\n",
    "df['Cuisine Keywords'] = cuisine_list\n",
    "df['Diet Keywords'] = diet_list\n",
    "df['Recipe URLs'] = urls\n",
    "\n",
    "df = df[[\"Title\", \"Rating\", \"Description\", \"Ingredients\", \"Recipe Facts\",\n",
    "         \"Directions\", \"Nutrition Facts\", 'Number of Servings',\n",
    "         'Calories', 'Total Fat', 'Saturated Fat', 'Cholesterol', 'Sodium',\n",
    "         'Total Carbohydrate', 'Dietary Fiber', 'Total Sugars', 'Protein',\n",
    "         'Vitamin C', 'Calcium', 'Iron', 'Potassium', \"Tags\", 'Course Keywords',\n",
    "         'Diet Keywords', 'Cuisine Keywords', 'Recipe URLs', 'Image Link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fbd23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to csv\n",
    "df.to_csv(\"recipes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to json\n",
    "df.to_json(\"recipes.json\", orient=\"index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ddfdadccf6e2849b76a147777c3b0c1c5ad990f0ff191b33320e636dd650b60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
